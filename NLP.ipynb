{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZSuJg3xFIay",
        "outputId": "846d0b74-f91b-4c9b-ca94-1f96419814c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jdxGELrFlEh",
        "outputId": "9da2356c-9cde-4fef-c89a-38d0fa5cff85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Hello, My name is AK.\n",
        "say my name\"\"\"\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDPdv9F8FLve",
        "outputId": "107185d1-839f-4420-935a-4a13333c71d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, My name is AK. \n",
            "say my name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenization\n",
        "## sentence --> paragraph\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "04vXa2MWFR4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "GPorlPfLFUHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "barmCZ4wFWPd",
        "outputId": "0ff022bf-99bc-48c7-9302-2805ca34af9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRjUj8Q1F3kD",
        "outputId": "0713caca-c207-4e5f-8d5e-4910073750af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, My name is AK.\n",
            "say my name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##tokenization\n",
        "##paragraph-->words\n",
        "##sentence-->words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Zfx9PI6xF8G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYTgHO-JGEyc",
        "outputId": "52442992-90e5-41f0-dac4-de95192246ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'My', 'name', 'is', 'AK', '.', 'say', 'my', 'name']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pISAwBNOGWAr",
        "outputId": "15378f19-682b-44d3-e666-869f22e5fec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'My', 'name', 'is', 'AK', '.']\n",
            "['say', 'my', 'name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n"
      ],
      "metadata": {
        "id": "Kdf5aawaGeFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uQNeRxVGmkw",
        "outputId": "e5edb171-3fa3-42bc-ffc1-852584948fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'My', 'name', 'is', 'AK', '.', 'say', 'my', 'name']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "LUUd-r7UGotg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "fKyJqrtYGu5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzRRMm6jG2GP",
        "outputId": "5bd05a06-4245-43b6-98ea-df5c58d928b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'My', 'name', 'is', 'AK.', 'say', 'my', 'name']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stemming\n",
        "##classification problem\n",
        "\n",
        "words=['eating', 'eats', 'eaten','writing', 'writes', 'programming', 'programs', 'history', 'finally', 'finalize' ]"
      ],
      "metadata": {
        "id": "ygMQveEQHC8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "-daLmfRvIuZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "2w3WsJyVI_cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"------\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peU_rjRNJDRZ",
        "outputId": "32a542c9-de2c-49ec-9983-3292fa046806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating------eat\n",
            "eats------eat\n",
            "eaten------eaten\n",
            "writing------write\n",
            "writes------write\n",
            "programming------program\n",
            "programs------program\n",
            "history------histori\n",
            "finally------final\n",
            "finalize------final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"congratulations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KwvZPPHGJLcR",
        "outputId": "39155af0-f143-4118-fe31-ef2dfcb42e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## regexpstemmer\n",
        "\n",
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "epvNkm5DJbWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing$|e$|ed$', min=4)"
      ],
      "metadata": {
        "id": "CzVgllBZJ23a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem(\"eating\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Eo-0E0bJ5fE",
        "outputId": "e9c48a97-2721-4e6c-8938-e660840067bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cxKg5ZghJ8O6",
        "outputId": "1184a9cf-ee68-49e9-d580-174d774c8684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## snowball stemmer"
      ],
      "metadata": {
        "id": "TGVvxnQWKFZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "QM9I3XNDKYK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowstem = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "JqJuZG77KcqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"------\"+snowstem.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ERWVqWKkkE",
        "outputId": "49145352-ae79-425f-acc9-c47d4debfa2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating------eat\n",
            "eats------eat\n",
            "eaten------eaten\n",
            "writing------write\n",
            "writes------write\n",
            "programming------program\n",
            "programs------program\n",
            "history------histori\n",
            "finally------final\n",
            "finalize------final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\"), stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57eCjKHVKob_",
        "outputId": "7af0d492-6c41-4e59-8c5d-0b0ee1c8c1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowstem.stem('fairly'), snowstem.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I39qj_wKxSP",
        "outputId": "7bc00ed2-c0a5-4b63-b5fa-0ef4213dbe4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "bRGeszwBK8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Rq6c2-Mtr2",
        "outputId": "abdf3c3d-4ff9-42b4-db17-2783a116d4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "QtkmKWleMY9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' pos -\n",
        "noun - n\n",
        "verb - v\n",
        "adverb - r\n",
        "adjective - a\n",
        "'''\n",
        "\n",
        "lemmatizer.lemmatize(\"going\", 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wGvSpiOrMczN",
        "outputId": "46eea6d7-bc76-42a4-8be4-8f81dc1bf33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"------\"+lemmatizer.lemmatize(word, pos='a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsabe_pgMjWJ",
        "outputId": "476ea952-a3c5-4661-b81a-e84d13b067dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating------eating\n",
            "eats------eats\n",
            "eaten------eaten\n",
            "writing------writing\n",
            "writes------writes\n",
            "programming------programming\n",
            "programs------programs\n",
            "history------history\n",
            "finally------finally\n",
            "finalize------finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\",'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8M0MDum7NLrU",
        "outputId": "1b5878f9-553e-468d-8a4f-3a93d10d3787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"fairly\", pos='n'), lemmatizer.lemmatize('sportingly', pos='n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X_F1mhaNj98",
        "outputId": "31e14a6b-379c-423f-b495-8229fb8d56b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stopwords\n",
        "\n",
        "paragraph= '''Dream, dream, dream. Dreams transform into thoughts, and thoughts result in action. When you dream, you have to dream big. Your dream will give you direction.\n",
        "Your goal will give you the energy to take action.\n",
        "If you see the glory of the country, if you see the beauty of our civilization, if you see our heritage, if you see the progress we’ve made, and then you see the future, it will give you immense energy.\n",
        "We need you to take up the responsibility of transforming this nation into a developed nation.\n",
        "\n",
        "The key factor for you to succeed is the education you receive, the attitude you develop, and the vision you have.\n",
        "You should have a vision, a roadmap for your life. What is your aim? It should not be just a career, it should be a mission in life.\n",
        "You should be driven by values and the will to serve society, which should be the essence of your life.\n",
        "\n",
        "A developed nation is a result of a commitment from its citizens.\n",
        "It is a nation of people who will always dream, always act, and always give their best.\n",
        "India should be a place where citizens believe in their ability to bring about change.\n",
        "We need young people like you to take the responsibility of making that change happen.\"\n",
        "\n",
        "\"To succeed in life and achieve results, you must understand and master the three essential components: Knowledge, Wisdom, and Human Values.\n",
        "Each of these helps in transforming your thoughts, actions, and success.\n",
        "Knowledge helps you understand the world around you, wisdom helps you make the right choices, and human values help you become a person who can serve others.\"\n",
        "\n",
        "\"India will be a developed nation only if we work together to build a strong foundation of creativity, innovation, and commitment.'''\n"
      ],
      "metadata": {
        "id": "jzaWlCrcNyot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "QNv_cS0oQwrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Ew6eqN5BQzvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaNk2f_PQ2bB",
        "outputId": "54e6ba26-2e26-49fe-e244-23b21a75f540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kALkQTbQ7A1",
        "outputId": "c0b89103-28b6-4c2f-c75a-cb0896363293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('german')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt6P5ccHQ_z_",
        "outputId": "b9085051-cbb4-42f8-9c97-7b08c092bea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aber',\n",
              " 'alle',\n",
              " 'allem',\n",
              " 'allen',\n",
              " 'aller',\n",
              " 'alles',\n",
              " 'als',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'ander',\n",
              " 'andere',\n",
              " 'anderem',\n",
              " 'anderen',\n",
              " 'anderer',\n",
              " 'anderes',\n",
              " 'anderm',\n",
              " 'andern',\n",
              " 'anderr',\n",
              " 'anders',\n",
              " 'auch',\n",
              " 'auf',\n",
              " 'aus',\n",
              " 'bei',\n",
              " 'bin',\n",
              " 'bis',\n",
              " 'bist',\n",
              " 'da',\n",
              " 'damit',\n",
              " 'dann',\n",
              " 'der',\n",
              " 'den',\n",
              " 'des',\n",
              " 'dem',\n",
              " 'die',\n",
              " 'das',\n",
              " 'dass',\n",
              " 'daß',\n",
              " 'derselbe',\n",
              " 'derselben',\n",
              " 'denselben',\n",
              " 'desselben',\n",
              " 'demselben',\n",
              " 'dieselbe',\n",
              " 'dieselben',\n",
              " 'dasselbe',\n",
              " 'dazu',\n",
              " 'dein',\n",
              " 'deine',\n",
              " 'deinem',\n",
              " 'deinen',\n",
              " 'deiner',\n",
              " 'deines',\n",
              " 'denn',\n",
              " 'derer',\n",
              " 'dessen',\n",
              " 'dich',\n",
              " 'dir',\n",
              " 'du',\n",
              " 'dies',\n",
              " 'diese',\n",
              " 'diesem',\n",
              " 'diesen',\n",
              " 'dieser',\n",
              " 'dieses',\n",
              " 'doch',\n",
              " 'dort',\n",
              " 'durch',\n",
              " 'ein',\n",
              " 'eine',\n",
              " 'einem',\n",
              " 'einen',\n",
              " 'einer',\n",
              " 'eines',\n",
              " 'einig',\n",
              " 'einige',\n",
              " 'einigem',\n",
              " 'einigen',\n",
              " 'einiger',\n",
              " 'einiges',\n",
              " 'einmal',\n",
              " 'er',\n",
              " 'ihn',\n",
              " 'ihm',\n",
              " 'es',\n",
              " 'etwas',\n",
              " 'euer',\n",
              " 'eure',\n",
              " 'eurem',\n",
              " 'euren',\n",
              " 'eurer',\n",
              " 'eures',\n",
              " 'für',\n",
              " 'gegen',\n",
              " 'gewesen',\n",
              " 'hab',\n",
              " 'habe',\n",
              " 'haben',\n",
              " 'hat',\n",
              " 'hatte',\n",
              " 'hatten',\n",
              " 'hier',\n",
              " 'hin',\n",
              " 'hinter',\n",
              " 'ich',\n",
              " 'mich',\n",
              " 'mir',\n",
              " 'ihr',\n",
              " 'ihre',\n",
              " 'ihrem',\n",
              " 'ihren',\n",
              " 'ihrer',\n",
              " 'ihres',\n",
              " 'euch',\n",
              " 'im',\n",
              " 'in',\n",
              " 'indem',\n",
              " 'ins',\n",
              " 'ist',\n",
              " 'jede',\n",
              " 'jedem',\n",
              " 'jeden',\n",
              " 'jeder',\n",
              " 'jedes',\n",
              " 'jene',\n",
              " 'jenem',\n",
              " 'jenen',\n",
              " 'jener',\n",
              " 'jenes',\n",
              " 'jetzt',\n",
              " 'kann',\n",
              " 'kein',\n",
              " 'keine',\n",
              " 'keinem',\n",
              " 'keinen',\n",
              " 'keiner',\n",
              " 'keines',\n",
              " 'können',\n",
              " 'könnte',\n",
              " 'machen',\n",
              " 'man',\n",
              " 'manche',\n",
              " 'manchem',\n",
              " 'manchen',\n",
              " 'mancher',\n",
              " 'manches',\n",
              " 'mein',\n",
              " 'meine',\n",
              " 'meinem',\n",
              " 'meinen',\n",
              " 'meiner',\n",
              " 'meines',\n",
              " 'mit',\n",
              " 'muss',\n",
              " 'musste',\n",
              " 'nach',\n",
              " 'nicht',\n",
              " 'nichts',\n",
              " 'noch',\n",
              " 'nun',\n",
              " 'nur',\n",
              " 'ob',\n",
              " 'oder',\n",
              " 'ohne',\n",
              " 'sehr',\n",
              " 'sein',\n",
              " 'seine',\n",
              " 'seinem',\n",
              " 'seinen',\n",
              " 'seiner',\n",
              " 'seines',\n",
              " 'selbst',\n",
              " 'sich',\n",
              " 'sie',\n",
              " 'ihnen',\n",
              " 'sind',\n",
              " 'so',\n",
              " 'solche',\n",
              " 'solchem',\n",
              " 'solchen',\n",
              " 'solcher',\n",
              " 'solches',\n",
              " 'soll',\n",
              " 'sollte',\n",
              " 'sondern',\n",
              " 'sonst',\n",
              " 'über',\n",
              " 'um',\n",
              " 'und',\n",
              " 'uns',\n",
              " 'unsere',\n",
              " 'unserem',\n",
              " 'unseren',\n",
              " 'unser',\n",
              " 'unseres',\n",
              " 'unter',\n",
              " 'viel',\n",
              " 'vom',\n",
              " 'von',\n",
              " 'vor',\n",
              " 'während',\n",
              " 'war',\n",
              " 'waren',\n",
              " 'warst',\n",
              " 'was',\n",
              " 'weg',\n",
              " 'weil',\n",
              " 'weiter',\n",
              " 'welche',\n",
              " 'welchem',\n",
              " 'welchen',\n",
              " 'welcher',\n",
              " 'welches',\n",
              " 'wenn',\n",
              " 'werde',\n",
              " 'werden',\n",
              " 'wie',\n",
              " 'wieder',\n",
              " 'will',\n",
              " 'wir',\n",
              " 'wird',\n",
              " 'wirst',\n",
              " 'wo',\n",
              " 'wollen',\n",
              " 'wollte',\n",
              " 'würde',\n",
              " 'würden',\n",
              " 'zu',\n",
              " 'zum',\n",
              " 'zur',\n",
              " 'zwar',\n",
              " 'zwischen']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "GxZxR1_eRKWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "4Y8E28oARXVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "tt8SlJRoRZvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTgddTqIRgKR",
        "outputId": "3fea1149-2106-4b97-f8e5-3f7c9ed7ffdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "nuPQEnUKR0Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcr8sz1rY4tB",
        "outputId": "7aefbe78-1317-4170-dc70-5e4f18bcc547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dream , dream , dream .',\n",
              " 'dream transform thought , thought result action .',\n",
              " 'when dream , dream big .',\n",
              " 'your dream give direct .',\n",
              " 'your goal give energi take action .',\n",
              " 'if see glori countri , see beauti civil , see heritag , see progress ’ made , see futur , give immens energi .',\n",
              " 'we need take respons transform nation develop nation .',\n",
              " 'the key factor succeed educ receiv , attitud develop , vision .',\n",
              " 'you vision , roadmap life .',\n",
              " 'what aim ?',\n",
              " 'it career , mission life .',\n",
              " 'you driven valu serv societi , essenc life .',\n",
              " 'a develop nation result commit citizen .',\n",
              " 'it nation peopl alway dream , alway act , alway give best .',\n",
              " 'india place citizen believ abil bring chang .',\n",
              " \"we need young peopl like take respons make chang happen . ''\",\n",
              " '`` to succeed life achiev result , must understand master three essenti compon : knowledg , wisdom , human valu .',\n",
              " 'each help transform thought , action , success .',\n",
              " \"knowledg help understand world around , wisdom help make right choic , human valu help becom person serv other . ''\",\n",
              " '`` india develop nation work togeth build strong foundat creativ , innov , commit .']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "acPCNQHcZHpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "goQTU5_FZ911"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[Lemmatizer.lemmatize(word.upper(),'v') for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)#converting all the words into sentences"
      ],
      "metadata": {
        "id": "Un_1bhsWaFsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXJhEicAaXSw",
        "outputId": "7b9fcaf4-6af7-4249-9424-9db1e3061678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DREAM , DREAM , DREAM .',\n",
              " 'DREAM TRANSFORM THINK , THINK RESULT ACTION .',\n",
              " 'DREAM , DREAM BIG .',\n",
              " 'DREAM GIVE DIRECT .',\n",
              " 'GOAL GIVE ENERGI TAKE ACTION .',\n",
              " 'SEE GLORI COUNTRI , SEE BEAUTI CIVIL , SEE HERITAG , SEE PROGRESS ’ MAKE , SEE FUTUR , GIVE IMMENS ENERGI .',\n",
              " 'NEED TAKE RESPONS TRANSFORM NATION DEVELOP NATION .',\n",
              " 'KEY FACTOR SUCCEED EDUC RECEIV , ATTITUD DEVELOP , VISION .',\n",
              " 'VISION , ROADMAP LIFE .',\n",
              " 'AIM ?',\n",
              " 'CAREER , MISSION LIFE .',\n",
              " 'DRIVE VALU SERV SOCIETI , ESSENC LIFE .',\n",
              " 'DEVELOP NATION RESULT COMMIT CITIZEN .',\n",
              " 'NATION PEOPL ALWAY DREAM , ALWAY ACT , ALWAY GIVE BEST .',\n",
              " 'INDIA PLACE CITIZEN BELIEV ABIL BRING CHANG .',\n",
              " 'NEED YOUNG PEOPL LIKE TAKE RESPONS MAKE CHANG HAPPEN . ``',\n",
              " '`` SUCCEED LIFE ACHIEV RESULT , MUST UNDERSTAND MASTER THREE ESSENTI COMPON : KNOWLEDG , WISDOM , HUMAN VALU .',\n",
              " 'HELP TRANSFORM THINK , ACTION , SUCCESS .',\n",
              " 'KNOWLEDG HELP UNDERSTAND WORLD AROUND , WISDOM HELP MAKE RIGHT CHOIC , HUMAN VALU HELP BECOM PERSON SERV . ``',\n",
              " '`` INDIA DEVELOP NATION WORK TOGETH BUILD STRONG FOUNDAT CREATIV , INNOV , COMMIT .']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2y76B-dcad2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}